{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to TensorFlow\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: Apr 27, 2021\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCES: \n",
    "\n",
    "- Learning PySpark, Chapter 8\n",
    "- TensorFlow documentation: https://www.tensorflow.org\n",
    "\n",
    "### OBJECTIVES\n",
    "-  Introduction to `TensorFlow`\n",
    "-  Review `TensorFlow` code and perform tensor calculations\n",
    "-  Understand the benefit of graph calculations\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Deep Learning\n",
    "- Tensors\n",
    "- Graph calculations\n",
    "- `Sessions` for `TensorFlow` graph calculations\n",
    "- Various tensor types including: `tf.constant`, `tf.Variable`, `tf.placeholder`\n",
    "\n",
    "---\n",
    "\n",
    "**Deep Learning**\n",
    "\n",
    "Deep Learning is part of family of ML techniques based on learning representations of data  \n",
    "\n",
    "Neural networks provide the basis for deep learning  \n",
    "\n",
    "The most general neural networks contain an *input layer*, one or more *hidden layers*, and an *output layer*.  The hidden layers are linear or nonlinear functions of the preceding layer. \n",
    "\n",
    "**Neural Network Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"neural_network1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models have specialized architectures, such as convolutional neural networks (CNNs), typically used in computer vision. CNNs take advantage of spatial structure in the image, and avoid using fully-connected layers as in the standard neural network.\n",
    "\n",
    "There are several deep learning use cases including:\n",
    "- speech recognition\n",
    "- facial recognition\n",
    "- computer vision\n",
    "- language translation\n",
    "- music composition\n",
    "\n",
    "There are several frameworks for machine learning/deep learning, including `TensorFlow`, `Theano`, `Torch`, `PyTorch`, `Caffe`, `CNTK`, `MXNet`, `DL4J`  \n",
    "\n",
    "\n",
    "\n",
    "**Deep learning has the potential to minimize or even replace manual feature engineering.**  This is because they are effective at learning very useful representations of the data.   \n",
    "\n",
    "**TensorFlow**  \n",
    "\n",
    "`TensorFlow` is Google's free and open-source software library for dataflow and differentiable programming across a range of tasks. It is a symbolic math library, and is also used for ML applications such as neural networks.\n",
    "\n",
    "For more details, refer to: https://www.tensorflow.org/\n",
    "\n",
    "**Tensors**  \n",
    "A *tensor* is a generalization of vectors and matrices to potentially higher dimensions.  \n",
    "`TensorFlow` represents tensors as n-dimensional arrays of base datatypes.  \n",
    "\n",
    "For details, I encourage you to read:  https://www.tensorflow.org/guide/tensors\n",
    "\n",
    "The fundamental object is the `tf.Tensor`.  `TensorFlow` programs create a graph which detail how a tensor will be computed based on other tensors. The `tf.Tensor` objects have a data type and a dimension or rank. This means all elements in the tensor must be the same, and the type must be defined upon initialization.\n",
    "\n",
    "There are special types of tensors, and they are:\n",
    "\n",
    "- `tf.Variable`\n",
    "- `tf.constant`\n",
    "- `tf.placeholder`\n",
    "- `tf.SparseTensor`\n",
    "\n",
    "**Rank or Dimension**\n",
    "\n",
    "| Rank |  Mathematical Object      |\n",
    "|----------|:-------------:|\n",
    "| 0 |  scalar |\n",
    "| 1 |  vector |\n",
    "| 2 |  matrix |\n",
    "| 3 |  3-tensor |\n",
    "| n |  n-tensor |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rank-1 `tf.Tensor` can be created like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vals  = tf.Variable([121.1, 454.99], tf.float32)\n",
    "vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow Data Flow Graphs**\n",
    "\n",
    " The figure below shows a data flow graph for an activation calculation commonly found in a neural network.  The `nodes` represent mathematical operations (`matmul`, `add`, `ReLU`), and the `edges`  are tensors (`W`, `b`, and `X`).\n",
    " \n",
    "<img src='tensorflow_graph1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow Sessions**\n",
    "\n",
    "`TensorFlow` uses the `Session` object to run the graph.  Below is a small example where we set up a session, define a vector of constants, and evaluate the `tf.constant` by calling the `run()` function on the session.\n",
    "\n",
    "**Evaluating a Tensor Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  tfc = tf.constant([3.14, 2.71])\n",
    "  tfc_sq = tfc * tfc\n",
    "  print(sess.run(tfc_sq))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix Multiplication Example**\n",
    "\n",
    "In this short example, we set up a $1x3$ matrix and a $3x1$ matrix, and then compute their product using the `matmul()` function.\n",
    "\n",
    "We use a `TensorFlow` session to do the computation.  Run this code and verify that you get the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Multiplication\n",
    "\n",
    "# 1x3 matrix\n",
    "c1 = tf.constant([[3., 2., 1.]])\n",
    "\n",
    "# 3x1 matrix\n",
    "c2 = tf.constant([[-1.], [2.], [1.]])\n",
    "\n",
    "# matrix multiplication\n",
    "p = tf.matmul(c1, c2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"matrix product: {}\".format(sess.run(p)))\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result:  \n",
    "matrix product: [[2.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Placeholder Tensors**\n",
    "\n",
    "We can use the `tf.placeholder` tensor to avoid fixing the values and sizes in advance.\n",
    "This makes the computation more flexible.  \n",
    "\n",
    "The parameter `feed_dict` provides a method for passing the data to the tensors in the graph.  \n",
    "It is a dictionary containing `key:value` pairs where the `key` is the tensor, and the `value` is the data for that tensor.  The example below sets up two tensors using placeholders, defines the data to populate the matrices, and uses `feed_dict` to pass the data to the tensors.\n",
    "\n",
    "Run code below and verify that you get the expected result.\n",
    "\n",
    "**Matrix Multiplication with Placeholders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = tf.placeholder(tf.float32)\n",
    "t2 = tf.placeholder(tf.float32)\n",
    "tp = tf.matmul(t1, t2)\n",
    "\n",
    "# now we define the matrices\n",
    "m1 = [[5., 2., 5.]]\n",
    "m2 = [[-1.], [2.], [1.]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "   print(sess.run([tp], feed_dict={t1:m1, t2:m2}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result:  \n",
    "[array([[4.]], dtype=float32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reusing the Graph**\n",
    "\n",
    "With placeholders, can easily reuse the same graph.  In the code cell below, create the following two matrices:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{M3} =  \\begin{bmatrix}\n",
    "1 & 1 & 1  \\\\\n",
    "1 & 0 & -1  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{M4} =  \\begin{bmatrix}\n",
    " 0 &  1 \\\\\n",
    " 1 &  1 \\\\\n",
    "-1 & -1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Next, compute their matrix product and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the matrix product of M3, M4 and print the result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result:  \n",
    "[array([[0., 1.],\n",
    "       [1., 2.]], dtype=float32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network Computation**\n",
    "\n",
    "In this example, we define activations $A$ from two training examples, and compute their product with weight tensor $W$.  There is no bias term $b$.  Next, we apply a `softmax` layer to produce probabilities for each of the two classes, from each of the training examples.\n",
    "\n",
    "The weight tensor $W$ is randomly initialized from a uniform distribution.  \n",
    "`seed=314` was set on the uniform generator for reproducability; please do not change this.  \n",
    "\n",
    "Since $W$ is a `tf.Variable` tensor, it must be initialized.  The typical way to initialize all variables is with the command  \n",
    "\n",
    "`init_op = tf.global_variables_initializer()`  \n",
    "\n",
    "The `init_op` needs to be run in the session.  Note that a session can be run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network computation\n",
    "\n",
    "A = tf.constant([[5.0, -2.0], [1.0, 8.0]])\n",
    "W = tf.Variable(tf.random_uniform([2, 2], seed=314))\n",
    "Y = tf.matmul(A, W)\n",
    "output = tf.nn.softmax(Y)\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  # Run the initializer on all variables\n",
    "  sess.run(init_op)\n",
    "\n",
    "  # Evaluate the neural net output (softmax predictions)\n",
    "  print(sess.run(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result:  \n",
    "[[0.87995726 0.12004273]  \n",
    " [0.17986473 0.82013524]]\n",
    " \n",
    " Note that the two values in each row are the softmax probabilities for an example, which sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introducing `Tensor2Tensor` Colab**\n",
    "\n",
    "This is a great online tool provided by Google for using `TensorFlow`.  \n",
    "Here is their short summary:\n",
    "\n",
    "*Tensor2Tensor, or T2T for short, is a library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research. T2T is actively used and maintained by researchers and engineers within the Google Brain team and a community of users. This colab shows you some datasets we have in T2T, how to download and use them, some models we have, how to download pre-trained models and use them, and how to create and train your own models.*\n",
    "\n",
    "Link to URL:  \n",
    "https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb#scrollTo=xGUjWehq8Vxq\n",
    "\n",
    "You can also paste your own `TensorFlow` code into cells and run them.  \n",
    "\n",
    "Be sure to import tensorflow at the start:\n",
    "\n",
    "`import tensorflow as tf`"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
